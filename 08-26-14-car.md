Schedule
--------

 - Use CoreRobot as a testing platfrom to transfer to Elanor as a platform to transfer to the large platform
 
Goal
----

 - Car drives
 - No accidents
 - No user input

Deliverables
------------

 - Can "pause" safely
 - Planner - KISS
 - Obstacle Detection
   - Static
   - Lead following?
 - Driver -- translates "path" to specific motor ocntrols
 - Lane detections
 
Timeline
--------

- Aug 26
  - Plan stuff
- Sep 1
  - Nothing necessary
- Sep 8
  - Nothing necessary
- September 15th
  - Sensors -- Camera
  - World Model -- Two line track
  - Planner -- Stay between lines
  - Driver -- Have a parameterized driver
  
- September 22nd (also this is the weekend of HackGT, for better or worse)
  - Sensors -- Camera, Lidar
  - World model -- Lines, "walls"
    - "Walls" are defined as "something that the lidar can reliably detect, be it people standing next to the road, curbs, etc."  If we can't reasonably assume that the lidar can detect such things, I think its best to fall back and scrap autonomous turning, because doing off of camera alone is something that I can't conceptualize
  - Planner -- Stay between walls
  - Driver -- make it better
  
- September 29th
  - Sensors -- Camera, Lidar
  - World model -- lines within walls, ie. a lane between walls
  - Path planner -- stay within the lane between walls, with more complex lanes/walls/curves
  - Driver -- continual improvements
  
- October 6th -- we get the car aroundish this time
  - Sensors -- Camera, Lidar, maybe add odometry since we have big car now?
  - World model -- lines, walls, locate/idenify static obstacles
  - Planner -- stuff from before + avoid obstacles
  - Driver -- contine making it better
  
- End
  - Sensors -- Camera, LIDAR, odametry?
  - Model -- Lines, "walls", Obstacles, localization?
  - Planner -- Stay between lines/in people pipe, avoid obstacles, turn
  - Driver -- Correctly convert high level instructions to machine code
